{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed79c38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "Installing datasets...\n",
      "Installing scikit-learn...\n",
      "‚úì All packages installed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing required packages...\")\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['datasets', 'scikit-learn', 'pandas', 'numpy']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "print(\"‚úì All packages installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1be3186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983e4c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING AMAZON ELECTRONICS REVIEWS DATASET\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING AMAZON ELECTRONICS REVIEWS DATASET\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f657fde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Electronics reviews from HuggingFace...\n",
      "This may take 2-3 minutes on first download...\n",
      "\n",
      "HuggingFace method failed: Dataset scripts are no longer supported, but found Amazon-Reviews-2023.py\n",
      "\n",
      "Trying alternative: Old Amazon Dataset (2014)...\n",
      "\n",
      "Downloading from UCSD...\n",
      "‚úÖ Loaded 10000 reviews from 2014 dataset\n",
      "\n",
      "Dataset Shape: (10000, 9)\n",
      "Columns: ['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', 'unixReviewTime', 'reviewTime']\n",
      "\n",
      "First 3 reviews:\n",
      "       reviewerID        asin     reviewerName   helpful  \\\n",
      "0   AO94DHGC771SJ  0528881469          amazdnu    [0, 0]   \n",
      "1   AMO214LNFCEI4  0528881469  Amazon Customer  [12, 15]   \n",
      "2  A3N7T0DY83Y4IG  0528881469    C. A. Freeman  [43, 45]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
      "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
      "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
      "\n",
      "             summary  unixReviewTime   reviewTime  \n",
      "0    Gotta have GPS!      1370131200   06 2, 2013  \n",
      "1  Very Disappointed      1290643200  11 25, 2010  \n",
      "2     1st impression      1283990400   09 9, 2010  \n"
     ]
    }
   ],
   "source": [
    "df = None\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"\\nLoading Electronics reviews from HuggingFace...\")\n",
    "    print(\"This may take 2-3 minutes on first download...\\n\")\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "        \"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "        \"raw_review_Electronics\",\n",
    "        split=\"full[:10000]\",\n",
    "        trust_remote_code=False  # Updated parameter\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(f\"‚úÖ Loaded {len(df)} reviews via HuggingFace\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"HuggingFace method failed: {e}\")\n",
    "    print(\"\\nTrying alternative: Old Amazon Dataset (2014)...\\n\")\n",
    "    \n",
    "    # Method 2: Old UCSD Dataset (backup)\n",
    "    try:\n",
    "        import gzip\n",
    "        import json\n",
    "        import urllib.request\n",
    "        \n",
    "        url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\"\n",
    "        \n",
    "        print(\"Downloading from UCSD...\")\n",
    "        urllib.request.urlretrieve(url, 'electronics.json.gz')\n",
    "        \n",
    "        reviews = []\n",
    "        with gzip.open('electronics.json.gz', 'rt', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 10000:\n",
    "                    break\n",
    "                reviews.append(json.loads(line))\n",
    "        \n",
    "        df = pd.DataFrame(reviews)\n",
    "        print(f\"‚úÖ Loaded {len(df)} reviews from 2014 dataset\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå All methods failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 reviews:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019a0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "‚úì Text column: 'reviewText'\n",
      "‚úì Rating column: 'overall'\n",
      "‚úì Removed 0 rows with missing values\n",
      "‚úì Final dataset: 10000 reviews\n",
      "\n",
      "üìä Class Distribution:\n",
      "   1-star:   572 (  5.7%)\n",
      "   2-star:   450 (  4.5%)\n",
      "   3-star:   822 (  8.2%)\n",
      "   4-star:  2095 ( 20.9%)\n",
      "   5-star:  6061 ( 60.6%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Identify column names\n",
    "text_col = None\n",
    "rating_col = None\n",
    "\n",
    "for col in ['text', 'reviewText', 'review_text']:\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "for col in ['rating', 'overall', 'stars']:\n",
    "    if col in df.columns:\n",
    "        rating_col = col\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úì Text column: '{text_col}'\")\n",
    "print(f\"‚úì Rating column: '{rating_col}'\")\n",
    "\n",
    "# Clean data\n",
    "df_clean = df[[text_col, rating_col]].dropna()\n",
    "print(f\"‚úì Removed {len(df) - len(df_clean)} rows with missing values\")\n",
    "\n",
    "# Convert ratings to integers\n",
    "df_clean[rating_col] = pd.to_numeric(df_clean[rating_col], errors='coerce')\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean[rating_col] = df_clean[rating_col].astype(int)\n",
    "df_clean = df_clean[df_clean[rating_col].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "print(f\"‚úì Final dataset: {len(df_clean)} reviews\")\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nüìä Class Distribution:\")\n",
    "dist = df_clean[rating_col].value_counts().sort_index()\n",
    "for rating, count in dist.items():\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f\"   {rating}-star: {count:5d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216aff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEXT PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Cleaning text...\n",
      "‚úì Text cleaned. Final reviews: 9991\n",
      "\n",
      "Example cleaned review:\n",
      "Original: We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipp...\n",
      "Cleaned:  we got this gps for my husband who is an otr over the road trucker very impressed with the shipping ...\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Text Cleaning\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"\\nCleaning text...\")\n",
    "df_clean['cleaned_text'] = df_clean[text_col].apply(clean_text)\n",
    "\n",
    "# Remove very short reviews\n",
    "df_clean = df_clean[df_clean['cleaned_text'].str.len() >= 10]\n",
    "print(f\"‚úì Text cleaned. Final reviews: {len(df_clean)}\")\n",
    "\n",
    "print(\"\\nExample cleaned review:\")\n",
    "print(f\"Original: {df_clean[text_col].iloc[0][:100]}...\")\n",
    "print(f\"Cleaned:  {df_clean['cleaned_text'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e3dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "Splitting data (80% train, 20% test)...\n",
      "Extracting TF-IDF features...\n",
      "‚úì Training set: 7992 samples\n",
      "‚úì Test set: 1999 samples\n",
      "‚úì Features: 1000 TF-IDF terms\n",
      "\n",
      "Top 10 features by TF-IDF score:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Cell 6: Feature Extraction\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X = df_clean['cleaned_text']\n",
    "y = df_clean[rating_col]\n",
    "\n",
    "# Train-test split\n",
    "print(\"\\nSplitting data (80% train, 20% test)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.8)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úì Training set: {X_train_tfidf.shape[0]} samples\")\n",
    "print(f\"‚úì Test set: {X_test_tfidf.shape[0]} samples\")\n",
    "print(f\"‚úì Features: {X_train_tfidf.shape[1]} TF-IDF terms\")\n",
    "\n",
    "print(\"\\nTop 10 features by TF-IDF score:\")\n",
    "feature_names = vectorizer.get_feature_names_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8abc2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING MODELS\n",
      "======================================================================\n",
      "\n",
      "[1/2] Training Naive Bayes...\n",
      "‚úì Naive Bayes trained\n",
      "\n",
      "[2/2] Training Logistic Regression...\n",
      "‚úì Logistic Regression trained\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 7: Train Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Model 1: Multinomial Naive Bayes\n",
    "print(\"\\n[1/2] Training Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "print(\"‚úì Naive Bayes trained\")\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "print(\"\\n[2/2] Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "print(\"‚úì Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a183a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìä MULTINOMIAL NAIVE BAYES:\n",
      "   Accuracy: 60.5% (0.6053)\n",
      "   MAE: 0.74\n",
      "\n",
      "üìä LOGISTIC REGRESSION (Nominal):\n",
      "   Accuracy: 62.6% (0.6258)\n",
      "   MAE: 0.62\n",
      "\n",
      "üìà Confusion Matrix - Naive Bayes:\n",
      "[[   0    0    0    0  114]\n",
      " [   0    0    0    1   89]\n",
      " [   0    0    0    1  164]\n",
      " [   0    0    0    0  419]\n",
      " [   0    0    0    1 1210]]\n",
      "\n",
      "üìà Confusion Matrix - Logistic Regression:\n",
      "[[  23    2    5    6   78]\n",
      " [   9    3    6   20   52]\n",
      " [   6    1    6   47  105]\n",
      " [   2    1    3   92  321]\n",
      " [   2    0    4   78 1127]]\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 8: Evaluate Results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate metrics\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "nb_mae = mean_absolute_error(y_test, nb_pred)\n",
    "nb_cm = confusion_matrix(y_test, nb_pred)\n",
    "\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "lr_cm = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "print(f\"\\nüìä MULTINOMIAL NAIVE BAYES:\")\n",
    "print(f\"   Accuracy: {nb_acc*100:.1f}% ({nb_acc:.4f})\")\n",
    "print(f\"   MAE: {nb_mae:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä LOGISTIC REGRESSION (Nominal):\")\n",
    "print(f\"   Accuracy: {lr_acc*100:.1f}% ({lr_acc:.4f})\")\n",
    "print(f\"   MAE: {lr_mae:.2f}\")\n",
    "\n",
    "# Confusion matrices\n",
    "print(\"\\nüìà Confusion Matrix - Naive Bayes:\")\n",
    "print(nb_cm)\n",
    "\n",
    "print(\"\\nüìà Confusion Matrix - Logistic Regression:\")\n",
    "print(lr_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867a0eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Adjacent Rating Errors:\n",
      "   Naive Bayes: 53.4% of errors are adjacent (e.g., 4‚Üî5)\n",
      "   Logistic Regression: 62.4% of errors are adjacent\n",
      "\n",
      "üìä Per-Class F1-Scores:\n",
      "\n",
      "Naive Bayes:\n",
      "   1-star: F1 = 0.000 (n=114)\n",
      "   2-star: F1 = 0.000 (n=90)\n",
      "   3-star: F1 = 0.000 (n=165)\n",
      "   4-star: F1 = 0.000 (n=419)\n",
      "   5-star: F1 = 0.755 (n=1211)\n",
      "\n",
      "Logistic Regression:\n",
      "   1-star: F1 = 0.295 (n=114)\n",
      "   2-star: F1 = 0.062 (n=90)\n",
      "   3-star: F1 = 0.063 (n=165)\n",
      "   4-star: F1 = 0.278 (n=419)\n",
      "   5-star: F1 = 0.779 (n=1211)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Adjacent rating error analysis\n",
    "def calc_adjacent_errors(cm):\n",
    "    \"\"\"Calculate % of errors that are adjacent ratings\"\"\"\n",
    "    total_errors = np.sum(cm) - np.trace(cm)\n",
    "    if total_errors == 0:\n",
    "        return 0\n",
    "    adjacent_errors = 0\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            if abs(i - j) == 1:\n",
    "                adjacent_errors += cm[i][j]\n",
    "    return (adjacent_errors / total_errors) * 100\n",
    "\n",
    "nb_adj = calc_adjacent_errors(nb_cm)\n",
    "lr_adj = calc_adjacent_errors(lr_cm)\n",
    "\n",
    "print(f\"\\nüìä Adjacent Rating Errors:\")\n",
    "print(f\"   Naive Bayes: {nb_adj:.1f}% of errors are adjacent (e.g., 4‚Üî5)\")\n",
    "print(f\"   Logistic Regression: {lr_adj:.1f}% of errors are adjacent\")\n",
    "\n",
    "# Per-class performance\n",
    "print(\"\\nüìä Per-Class F1-Scores:\")\n",
    "nb_report = classification_report(y_test, nb_pred, output_dict=True, zero_division=0)\n",
    "lr_report = classification_report(y_test, lr_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "print(\"\\nNaive Bayes:\")\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    if str(rating) in nb_report:\n",
    "        f1 = nb_report[str(rating)]['f1-score']\n",
    "        support = nb_report[str(rating)]['support']\n",
    "        print(f\"   {rating}-star: F1 = {f1:.3f} (n={int(support)})\")\n",
    "\n",
    "print(\"\\nLogistic Regression:\")\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    if str(rating) in lr_report:\n",
    "        f1 = lr_report[str(rating)]['f1-score']\n",
    "        support = lr_report[str(rating)]['support']\n",
    "        print(f\"   {rating}-star: F1 = {f1:.3f} (n={int(support)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f00dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìù COPY THIS INTO YOUR PROPOSAL - PRELIMINARY RESULTS SECTION\n",
      "======================================================================\n",
      "\n",
      "Initial experiments on 9,991 Electronics reviews show promising \n",
      "directions. Using TF-IDF features (max 1,000 features), Multinomial Naive Bayes \n",
      "achieved 60.5% accuracy with MAE of 0.74, while Logistic Regression \n",
      "(nominal treatment) achieved 62.6% accuracy with MAE of 0.62. \n",
      "Confusion matrix analysis reveals that 62% of misclassifications occur \n",
      "between adjacent ratings (particularly 4‚Üî5 stars), supporting our hypothesis that \n",
      "ordinal treatment may improve performance. The class imbalance is evident‚Äîmodels \n",
      "achieve 78% F1-score for 5-star reviews but only 6% \n",
      "for 3-star reviews. These preliminary findings motivate our investigation into \n",
      "whether ordinal methods can reduce MAE by better modeling rating structure while \n",
      "addressing the adjacent-rating confusion problem.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìã Next Steps:\n",
      "   1. Copy the paragraph above\n",
      "   2. Replace the placeholder in your proposal's 'Preliminary Results' section\n",
      "   3. Add your 2 groupmates' names to the proposal\n",
      "   4. Export proposal to PDF\n",
      "   5. Submit!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 10: Generate Proposal Paragraph\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìù COPY THIS INTO YOUR PROPOSAL - PRELIMINARY RESULTS SECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get key metrics\n",
    "f1_5star = lr_report.get('5', {}).get('f1-score', 0)\n",
    "f1_3star = lr_report.get('3', {}).get('f1-score', 0)\n",
    "\n",
    "paragraph = f\"\"\"Initial experiments on {len(df_clean):,} Electronics reviews show promising \n",
    "directions. Using TF-IDF features (max 1,000 features), Multinomial Naive Bayes \n",
    "achieved {nb_acc*100:.1f}% accuracy with MAE of {nb_mae:.2f}, while Logistic Regression \n",
    "(nominal treatment) achieved {lr_acc*100:.1f}% accuracy with MAE of {lr_mae:.2f}. \n",
    "Confusion matrix analysis reveals that {lr_adj:.0f}% of misclassifications occur \n",
    "between adjacent ratings (particularly 4‚Üî5 stars), supporting our hypothesis that \n",
    "ordinal treatment may improve performance. The class imbalance is evident‚Äîmodels \n",
    "achieve {f1_5star*100:.0f}% F1-score for 5-star reviews but only {f1_3star*100:.0f}% \n",
    "for 3-star reviews. These preliminary findings motivate our investigation into \n",
    "whether ordinal methods can reduce MAE by better modeling rating structure while \n",
    "addressing the adjacent-rating confusion problem.\"\"\"\n",
    "\n",
    "print(\"\\n\" + paragraph)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. Copy the paragraph above\")\n",
    "print(\"   2. Replace the placeholder in your proposal's 'Preliminary Results' section\")\n",
    "print(\"   3. Add your 2 groupmates' names to the proposal\")\n",
    "print(\"   4. Export proposal to PDF\")\n",
    "print(\"   5. Submit!\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
