{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Notebook 3: Nominal Models\n",
    "## Final Project - Ordinal vs Nominal Sentiment Analysis\n",
    "### Atharv Chaudhary\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose:** Train and evaluate NOMINAL classification models.\n",
    "\n",
    "**Models:**\n",
    "1. Multinomial Naive Bayes\n",
    "2. Logistic Regression (Multinomial)\n",
    "\n",
    "**Input:** `amazon_electronics_cleaned.csv`\n",
    "\n",
    "**Output:** `nominal_results.csv`, confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    mean_absolute_error, \n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Settings\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('amazon_electronics_cleaned.csv')\n",
    "print(f\"‚úÖ Loaded {len(df):,} reviews\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nüìä Rating Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TF-IDF FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TF-IDF FEATURE EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "MAX_FEATURES = 5000\n",
    "NGRAM_RANGE = (1, 2)  # Unigrams and bigrams\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    stop_words='english',\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"   Max features: {MAX_FEATURES}\")\n",
    "print(f\"   N-gram range: {NGRAM_RANGE}\")\n",
    "\n",
    "# Transform\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['rating'].values\n",
    "\n",
    "print(f\"\\n‚úÖ TF-IDF matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"‚úÖ Test set: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"üìä {model_name}\")\n",
    "    print(f\"{'='*55}\")\n",
    "    print(f\"Accuracy:      {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"MAE:           {mae:.4f}\")\n",
    "    print(f\"F1 (macro):    {f1_macro:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'encoding': 'Nominal',\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_error_rates(y_true, y_pred):\n",
    "    \"\"\"Calculate adjacent and severe error rates.\"\"\"\n",
    "    errors = y_true != y_pred\n",
    "    if errors.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    error_distances = np.abs(y_true[errors] - y_pred[errors])\n",
    "    adjacent = (error_distances == 1).sum() / errors.sum()\n",
    "    severe = (error_distances >= 2).sum() / errors.sum()\n",
    "    \n",
    "    return adjacent, severe\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model 1 - Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: MULTINOMIAL NAIVE BAYES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß MODEL 1: Multinomial Naive Bayes (Nominal)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTreats classes as UNORDERED categories.\")\n",
    "print(\"Formula: P(Y=k|x) ‚àù P(Y=k) √ó Œ† P(x‚±º|Y=k)\")\n",
    "\n",
    "# Train\n",
    "nb_model = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "nb_results = evaluate_model(y_test, nb_pred, \"Naive Bayes\")\n",
    "\n",
    "# Error analysis\n",
    "nb_adjacent, nb_severe = calculate_error_rates(y_test, nb_pred)\n",
    "nb_results['adjacent_error'] = nb_adjacent\n",
    "nb_results['severe_error'] = nb_severe\n",
    "\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"   Adjacent Error Rate (¬±1): {nb_adjacent:.2%}\")\n",
    "print(f\"   Severe Error Rate (¬±2+):  {nb_severe:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nüìã Classification Report - Naive Bayes:\")\n",
    "print(classification_report(y_test, nb_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model 2 - Logistic Regression (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION (MULTINOMIAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß MODEL 2: Logistic Regression (Nominal - Multinomial)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUses softmax, treats classes as UNORDERED.\")\n",
    "print(\"Formula: P(Y=k|x) = exp(w‚Çñ·µÄx + b‚Çñ) / Œ£ exp(w‚±º·µÄx + b‚±º)\")\n",
    "\n",
    "# Train\n",
    "lr_model = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_model(y_test, lr_pred, \"Logistic Regression\")\n",
    "\n",
    "# Error analysis\n",
    "lr_adjacent, lr_severe = calculate_error_rates(y_test, lr_pred)\n",
    "lr_results['adjacent_error'] = lr_adjacent\n",
    "lr_results['severe_error'] = lr_severe\n",
    "\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"   Adjacent Error Rate (¬±1): {lr_adjacent:.2%}\")\n",
    "print(f\"   Severe Error Rate (¬±2+):  {lr_severe:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nüìã Classification Report - Logistic Regression:\")\n",
    "print(classification_report(y_test, lr_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFUSION MATRICES\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Naive Bayes\n",
    "cm_nb = confusion_matrix(y_test, nb_pred)\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "axes[0].set_xlabel('Predicted Rating', fontsize=11)\n",
    "axes[0].set_ylabel('Actual Rating', fontsize=11)\n",
    "axes[0].set_title(f'Naive Bayes (Nominal)\\nAccuracy: {nb_results[\"accuracy\"]:.2%}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "\n",
    "# Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "axes[1].set_xlabel('Predicted Rating', fontsize=11)\n",
    "axes[1].set_ylabel('Actual Rating', fontsize=11)\n",
    "axes[1].set_title(f'Logistic Regression (Nominal)\\nAccuracy: {lr_results[\"accuracy\"]:.2%}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_nominal.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Saved: confusion_matrices_nominal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Combine results\n",
    "nominal_results = pd.DataFrame([nb_results, lr_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä NOMINAL MODELS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(nominal_results.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "nominal_results.to_csv('nominal_results.csv', index=False)\n",
    "print(\"\\n‚úÖ Saved: nominal_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for later analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'nb_pred': nb_pred,\n",
    "    'lr_pred': lr_pred\n",
    "})\n",
    "predictions_df.to_csv('nominal_predictions.csv', index=False)\n",
    "print(\"‚úÖ Saved: nominal_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('nominal_results.csv')\n",
    "    files.download('confusion_matrices_nominal.png')\n",
    "except:\n",
    "    print(\"Files saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Summary\n",
    "\n",
    "**Nominal Models Trained:**\n",
    "\n",
    "| Model | Accuracy | MAE | Adjacent Error | Severe Error |\n",
    "|-------|----------|-----|----------------|---------------|\n",
    "| Naive Bayes | See above | See above | See above | See above |\n",
    "| Logistic Regression | See above | See above | See above | See above |\n",
    "\n",
    "**Key Observation:** Both models treat ratings as unordered categories, potentially missing ordinal structure.\n",
    "\n",
    "**Next:** Run `4_Models_Ordinal.ipynb` to compare with ordinal methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
