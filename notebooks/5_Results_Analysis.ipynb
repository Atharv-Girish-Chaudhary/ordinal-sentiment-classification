{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Notebook 5: Results Analysis & Comparison\n",
    "## Final Project - Ordinal vs Nominal Sentiment Analysis\n",
    "### Atharv Chaudhary\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose:** Compare all models, create final visualizations for report.\n",
    "\n",
    "**Input:** `nominal_results.csv`, `ordinal_results.csv`\n",
    "\n",
    "**Output:** Final comparison charts, results table for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "print(\"âœ… Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from Notebooks 3 and 4\n",
    "nominal_results = pd.read_csv('nominal_results.csv')\n",
    "ordinal_results = pd.read_csv('ordinal_results.csv')\n",
    "\n",
    "# Combine\n",
    "all_results = pd.concat([nominal_results, ordinal_results], ignore_index=True)\n",
    "\n",
    "print(\"âœ… Loaded all results\")\n",
    "print(\"\\nğŸ“Š Complete Results:\")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FORMATTED RESULTS TABLE (For Report)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“‹ RESULTS TABLE FOR REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Format for display\n",
    "display_df = all_results.copy()\n",
    "display_df['accuracy'] = display_df['accuracy'].apply(lambda x: f\"{x:.2%}\")\n",
    "display_df['mae'] = display_df['mae'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['f1_macro'] = display_df['f1_macro'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['adjacent_error'] = display_df['adjacent_error'].apply(lambda x: f\"{x:.1%}\")\n",
    "display_df['severe_error'] = display_df['severe_error'].apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "# Select columns for report\n",
    "report_table = display_df[['model', 'encoding', 'accuracy', 'mae', 'f1_macro', 'adjacent_error', 'severe_error']]\n",
    "report_table.columns = ['Model', 'Encoding', 'Accuracy', 'MAE', 'F1 Macro', 'Adj. Error', 'Severe Error']\n",
    "\n",
    "print(report_table.to_string(index=False))\n",
    "\n",
    "# Save for report\n",
    "report_table.to_csv('final_results_table.csv', index=False)\n",
    "print(\"\\nâœ… Saved: final_results_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models = ['NB\\n(Nominal)', 'LR\\n(Nominal)', 'Ridge\\n(Ordinal)', 'OLR\\n(Ordinal)']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "# Panel 1: Accuracy\n",
    "bars = axes[0, 0].bar(range(len(models)), all_results['accuracy'], color=colors, edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(models)))\n",
    "axes[0, 0].set_xticklabels(models, fontsize=10)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Accuracy (Higher = Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(all_results['accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.2%}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Panel 2: MAE\n",
    "bars = axes[0, 1].bar(range(len(models)), all_results['mae'], color=colors, edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(models)))\n",
    "axes[0, 1].set_xticklabels(models, fontsize=10)\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error', fontsize=11)\n",
    "axes[0, 1].set_title('MAE (Lower = Better)', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(all_results['mae']):\n",
    "    axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Panel 3: F1 Macro\n",
    "bars = axes[1, 0].bar(range(len(models)), all_results['f1_macro'], color=colors, edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(models)))\n",
    "axes[1, 0].set_xticklabels(models, fontsize=10)\n",
    "axes[1, 0].set_ylabel('F1 Score (Macro)', fontsize=11)\n",
    "axes[1, 0].set_title('F1 Macro (Higher = Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(all_results['f1_macro']):\n",
    "    axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Panel 4: Error Types\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "bars1 = axes[1, 1].bar(x - width/2, all_results['adjacent_error'], width, \n",
    "                       label='Adjacent (Â±1)', color='#f39c12', edgecolor='black')\n",
    "bars2 = axes[1, 1].bar(x + width/2, all_results['severe_error'], width, \n",
    "                       label='Severe (Â±2+)', color='#c0392b', edgecolor='black')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(models, fontsize=10)\n",
    "axes[1, 1].set_ylabel('Error Rate (% of errors)', fontsize=11)\n",
    "axes[1, 1].set_title('Error Type Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(loc='upper right')\n",
    "axes[1, 1].set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Saved: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Ordinal vs Nominal Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ORDINAL VS NOMINAL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Calculate averages by encoding type\n",
    "nominal_data = all_results[all_results['encoding'] == 'Nominal']\n",
    "ordinal_data = all_results[all_results['encoding'] == 'Ordinal']\n",
    "\n",
    "# Panel 1: Metrics comparison\n",
    "metrics = ['Accuracy', 'MAE', 'F1 Macro']\n",
    "nominal_vals = [nominal_data['accuracy'].mean(), nominal_data['mae'].mean(), nominal_data['f1_macro'].mean()]\n",
    "ordinal_vals = [ordinal_data['accuracy'].mean(), ordinal_data['mae'].mean(), ordinal_data['f1_macro'].mean()]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, nominal_vals, width, label='Nominal', color='#3498db', edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, ordinal_vals, width, label='Ordinal', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics, fontsize=11)\n",
    "axes[0].set_ylabel('Score', fontsize=11)\n",
    "axes[0].set_title('Average Metrics by Encoding Type', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "for i, (n, o) in enumerate(zip(nominal_vals, ordinal_vals)):\n",
    "    axes[0].text(i - width/2, n + 0.02, f'{n:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    axes[0].text(i + width/2, o + 0.02, f'{o:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Panel 2: Error type comparison\n",
    "error_types = ['Adjacent (Â±1)', 'Severe (Â±2+)']\n",
    "nominal_errors = [nominal_data['adjacent_error'].mean(), nominal_data['severe_error'].mean()]\n",
    "ordinal_errors = [ordinal_data['adjacent_error'].mean(), ordinal_data['severe_error'].mean()]\n",
    "\n",
    "x = np.arange(len(error_types))\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, nominal_errors, width, label='Nominal', color='#3498db', edgecolor='black')\n",
    "bars2 = axes[1].bar(x + width/2, ordinal_errors, width, label='Ordinal', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(error_types, fontsize=11)\n",
    "axes[1].set_ylabel('Error Rate', fontsize=11)\n",
    "axes[1].set_title('Error Distribution by Encoding Type', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "for i, (n, o) in enumerate(zip(nominal_errors, ordinal_errors)):\n",
    "    axes[1].text(i - width/2, n + 0.01, f'{n:.1%}', ha='center', fontsize=9, fontweight='bold')\n",
    "    axes[1].text(i + width/2, o + 0.01, f'{o:.1%}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ordinal_vs_nominal.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Saved: ordinal_vs_nominal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KEY FINDINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best models\n",
    "best_accuracy = all_results.loc[all_results['accuracy'].idxmax()]\n",
    "best_mae = all_results.loc[all_results['mae'].idxmin()]\n",
    "lowest_severe = all_results.loc[all_results['severe_error'].idxmin()]\n",
    "\n",
    "print(f\"\\n1. BEST ACCURACY:\")\n",
    "print(f\"   {best_accuracy['model']} ({best_accuracy['encoding']})\")\n",
    "print(f\"   Accuracy: {best_accuracy['accuracy']:.2%}\")\n",
    "\n",
    "print(f\"\\n2. LOWEST MAE (Best Ordinal Performance):\")\n",
    "print(f\"   {best_mae['model']} ({best_mae['encoding']})\")\n",
    "print(f\"   MAE: {best_mae['mae']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. LOWEST SEVERE ERROR RATE:\")\n",
    "print(f\"   {lowest_severe['model']} ({lowest_severe['encoding']})\")\n",
    "print(f\"   Severe Error Rate: {lowest_severe['severe_error']:.2%}\")\n",
    "\n",
    "# Ordinal vs Nominal comparison\n",
    "nominal_avg_mae = nominal_data['mae'].mean()\n",
    "ordinal_avg_mae = ordinal_data['mae'].mean()\n",
    "mae_improvement = (nominal_avg_mae - ordinal_avg_mae) / nominal_avg_mae * 100\n",
    "\n",
    "nominal_avg_severe = nominal_data['severe_error'].mean()\n",
    "ordinal_avg_severe = ordinal_data['severe_error'].mean()\n",
    "severe_reduction = (nominal_avg_severe - ordinal_avg_severe) / nominal_avg_severe * 100\n",
    "\n",
    "print(f\"\\n4. ORDINAL VS NOMINAL COMPARISON:\")\n",
    "print(f\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"   â”‚ Metric          â”‚ Nominal â”‚ Ordinal   â”‚\")\n",
    "print(f\"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"   â”‚ Avg MAE         â”‚ {nominal_avg_mae:.4f}  â”‚ {ordinal_avg_mae:.4f}    â”‚\")\n",
    "print(f\"   â”‚ Avg Severe Err  â”‚ {nominal_avg_severe:.2%}  â”‚ {ordinal_avg_severe:.2%}    â”‚\")\n",
    "print(f\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(f\"\\n   âœ… MAE Improvement: {mae_improvement:.2f}%\")\n",
    "print(f\"   âœ… Severe Error Reduction: {severe_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY FOR REPORT (Copy this!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“„ SUMMARY FOR REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "RESEARCH QUESTION:\n",
    "Do the performance gains from ordinal treatment of 5-star ratings \n",
    "justify the increased model complexity?\n",
    "\n",
    "DATASET:\n",
    "- Amazon Electronics Reviews (McAuley Lab, UCSD)\n",
    "- Features: TF-IDF (5,000 features, unigrams + bigrams)\n",
    "\n",
    "MODELS COMPARED:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model                   â”‚ Encoding â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Multinomial Naive Bayes â”‚ Nominal  â”‚\n",
    "â”‚ Logistic Regression     â”‚ Nominal  â”‚\n",
    "â”‚ Ridge Regression        â”‚ Ordinal  â”‚\n",
    "â”‚ Ordinal Logistic Reg    â”‚ Ordinal  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "KEY RESULTS:\n",
    "- Best Accuracy: {best_accuracy['model']} ({best_accuracy['accuracy']:.2%})\n",
    "- Lowest MAE: {best_mae['model']} ({best_mae['mae']:.4f})\n",
    "- Lowest Severe Error: {lowest_severe['model']} ({lowest_severe['severe_error']:.2%})\n",
    "\n",
    "ORDINAL VS NOMINAL:\n",
    "- MAE Improvement: {mae_improvement:.2f}%\n",
    "- Severe Error Reduction: {severe_reduction:.2f}%\n",
    "\n",
    "CONCLUSION:\n",
    "Ordinal methods reduce MAE by {mae_improvement:.1f}% and severe errors \n",
    "by {severe_reduction:.1f}%, supporting the hypothesis that ordinal \n",
    "treatment improves sentiment classification quality.\n",
    "\n",
    "The performance gains justify the model complexity when:\n",
    "1. Minimizing severe misclassifications is important\n",
    "2. The ordinal structure of ratings is meaningful\n",
    "3. User experience depends on prediction accuracy\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "print(\"\\nğŸ“¥ Output Files:\")\n",
    "print(\"   - final_results_table.csv\")\n",
    "print(\"   - model_comparison.png\")\n",
    "print(\"   - ordinal_vs_nominal.png\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('final_results_table.csv')\n",
    "    files.download('model_comparison.png')\n",
    "    files.download('ordinal_vs_nominal.png')\n",
    "    print(\"\\nâœ… Downloads complete!\")\n",
    "except:\n",
    "    print(\"\\nFiles saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Project Complete!\n",
    "\n",
    "### Output Files:\n",
    "1. `class_distribution.png` - For Dataset section\n",
    "2. `confusion_matrices_nominal.png` - Nominal model results\n",
    "3. `confusion_matrices_ordinal.png` - Ordinal model results  \n",
    "4. `model_comparison.png` - Main comparison figure\n",
    "5. `ordinal_vs_nominal.png` - Encoding comparison\n",
    "6. `final_results_table.csv` - Results table for report\n",
    "\n",
    "### For Report:\n",
    "- Use the summary text above in your Discussion section\n",
    "- Include all figures in Results section\n",
    "- Cite the dataset properly\n",
    "\n",
    "### Citation:\n",
    "```\n",
    "@article{hou2024bridging,\n",
    "  title={Bridging Language and Items for Retrieval and Recommendation},\n",
    "  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},\n",
    "  journal={arXiv preprint arXiv:2403.03952},\n",
    "  year={2024}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
