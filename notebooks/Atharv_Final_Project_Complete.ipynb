{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Final Project: Ordinal vs Nominal Sentiment Analysis\n",
    "## Atharv Chaudhary - MSAI Class of 2027\n",
    "\n",
    "**Research Question:** Do the performance gains from ordinal treatment justify the increased model complexity?\n",
    "\n",
    "**Models:**\n",
    "1. Multinomial Naive Bayes (Nominal Baseline)\n",
    "2. Logistic Regression (Nominal - Multinomial)\n",
    "3. Ridge Regression (Ordinal)\n",
    "4. Ordinal Logistic Regression (Threshold-Based)\n",
    "\n",
    "**Dataset:** Amazon Electronics Reviews (UCSD/McAuley Lab)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install datasets mord -q\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    mean_absolute_error, \n",
    "    mean_squared_error,\n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# For loading Amazon dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"âœ… All libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon Reviews Dataset from HuggingFace\n",
    "print(\"Loading Amazon Electronics Reviews dataset...\")\n",
    "print(\"âš ï¸ This may take a few minutes for the first time.\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    \"raw_review_Electronics\",\n",
    "    split=\"full\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "df_full = dataset.to_pandas()\n",
    "print(f\"âœ… Loaded {len(df_full):,} reviews\")\n",
    "print(f\"Columns: {df_full.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and clean data\n",
    "SAMPLE_SIZE = 50000  # Adjust based on compute resources\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Sampling {SAMPLE_SIZE:,} reviews with stratification...\")\n",
    "\n",
    "# Convert rating to int\n",
    "df_full['rating_int'] = df_full['rating'].astype(int)\n",
    "\n",
    "# Stratified sampling\n",
    "df = df_full.groupby('rating_int', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE // 5), random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['text', 'rating']].copy()\n",
    "df = df.dropna(subset=['text', 'rating'])\n",
    "df = df[df['text'].str.len() >= 10]\n",
    "df['rating'] = df['rating'].astype(int)\n",
    "df = df[df['rating'].between(1, 5)]\n",
    "\n",
    "print(f\"âœ… After cleaning: {len(df):,} reviews\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rating_counts = df['rating'].value_counts().sort_index()\n",
    "colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#27ae60']\n",
    "\n",
    "bars = ax.bar(rating_counts.index, rating_counts.values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "for bar, count in zip(bars, rating_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "            f'{count:,}\\n({count/len(df)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Star Rating', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Number of Reviews', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Class Distribution of Amazon Electronics Reviews', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([1, 2, 3, 4, 5])\n",
    "ax.set_xticklabels(['1 â­', '2 â­', '3 â­', '4 â­', '5 â­'], fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['rating'].values\n",
    "\n",
    "print(f\"âœ… TF-IDF matrix shape: {X.shape}\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"âœ… Test set: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ðŸ“Š {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:      {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"MAE:           {mae:.4f}\")\n",
    "    print(f\"MSE:           {mse:.4f}\")\n",
    "    print(f\"F1 (macro):    {f1_macro:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "def calculate_adjacent_error_rate(y_true, y_pred):\n",
    "    \"\"\"Calculate percentage of errors between adjacent ratings.\"\"\"\n",
    "    errors = y_true != y_pred\n",
    "    if errors.sum() == 0:\n",
    "        return 0.0\n",
    "    adjacent_errors = np.abs(y_true[errors] - y_pred[errors]) == 1\n",
    "    return adjacent_errors.sum() / errors.sum()\n",
    "\n",
    "def calculate_severe_error_rate(y_true, y_pred):\n",
    "    \"\"\"Calculate percentage of errors with distance >= 2.\"\"\"\n",
    "    errors = y_true != y_pred\n",
    "    if errors.sum() == 0:\n",
    "        return 0.0\n",
    "    severe_errors = np.abs(y_true[errors] - y_pred[errors]) >= 2\n",
    "    return severe_errors.sum() / errors.sum()\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 1: Multinomial Naive Bayes (Nominal Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Multinomial Naive Bayes...\")\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "nb_results = evaluate_model(y_test, nb_pred, \"Naive Bayes (Nominal)\")\n",
    "nb_adjacent = calculate_adjacent_error_rate(y_test, nb_pred)\n",
    "nb_severe = calculate_severe_error_rate(y_test, nb_pred)\n",
    "print(f\"Adjacent Error Rate: {nb_adjacent:.2%}\")\n",
    "print(f\"Severe Error Rate:   {nb_severe:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, nb_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: Logistic Regression (Nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression (Nominal)...\")\n",
    "\n",
    "lr_nominal = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_nominal.fit(X_train, y_train)\n",
    "lr_nom_pred = lr_nominal.predict(X_test)\n",
    "\n",
    "lr_nom_results = evaluate_model(y_test, lr_nom_pred, \"Logistic Regression (Nominal)\")\n",
    "lr_nom_adjacent = calculate_adjacent_error_rate(y_test, lr_nom_pred)\n",
    "lr_nom_severe = calculate_severe_error_rate(y_test, lr_nom_pred)\n",
    "print(f\"Adjacent Error Rate: {lr_nom_adjacent:.2%}\")\n",
    "print(f\"Severe Error Rate:   {lr_nom_severe:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_nom_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 3: Ridge Regression (Ordinal)\n",
    "\n",
    "This model treats ratings as **continuous ordinal values** (1 < 2 < 3 < 4 < 5), then rounds to nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Ridge Regression (Ordinal)...\")\n",
    "print(\"Treats ratings as continuous: 1 < 2 < 3 < 4 < 5\")\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_pred_continuous = ridge_model.predict(X_test)\n",
    "ridge_pred = np.clip(np.round(ridge_pred_continuous), 1, 5).astype(int)\n",
    "\n",
    "ridge_results = evaluate_model(y_test, ridge_pred, \"Ordinal Regression (Ridge)\")\n",
    "ridge_adjacent = calculate_adjacent_error_rate(y_test, ridge_pred)\n",
    "ridge_severe = calculate_severe_error_rate(y_test, ridge_pred)\n",
    "print(f\"Adjacent Error Rate: {ridge_adjacent:.2%}\")\n",
    "print(f\"Severe Error Rate:   {ridge_severe:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, ridge_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model 4: Ordinal Logistic Regression (Threshold-Based)\n",
    "\n",
    "This implements the **proportional odds model** using K-1 binary classifiers for K classes.\n",
    "\n",
    "For each threshold k, we train a classifier for P(Y â‰¤ k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalLogisticRegression:\n",
    "    \"\"\"\n",
    "    Ordinal Logistic Regression using threshold approach.\n",
    "    Trains K-1 binary classifiers for K classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=1000):\n",
    "        self.max_iter = max_iter\n",
    "        self.classifiers = []\n",
    "        self.classes_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Train K-1 binary classifiers\n",
    "        for k in range(n_classes - 1):\n",
    "            threshold = self.classes_[k]\n",
    "            # Binary labels: 1 if y <= threshold, 0 otherwise\n",
    "            y_binary = (y <= threshold).astype(int)\n",
    "            \n",
    "            clf = LogisticRegression(max_iter=self.max_iter, random_state=42)\n",
    "            clf.fit(X, y_binary)\n",
    "            self.classifiers.append(clf)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Get cumulative probabilities P(Y <= k)\n",
    "        cumulative_probs = np.zeros((n_samples, n_classes))\n",
    "        cumulative_probs[:, -1] = 1.0  # P(Y <= max_class) = 1\n",
    "        \n",
    "        for k, clf in enumerate(self.classifiers):\n",
    "            cumulative_probs[:, k] = clf.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Convert to class probabilities P(Y = k)\n",
    "        class_probs = np.zeros((n_samples, n_classes))\n",
    "        class_probs[:, 0] = cumulative_probs[:, 0]\n",
    "        for k in range(1, n_classes):\n",
    "            class_probs[:, k] = cumulative_probs[:, k] - cumulative_probs[:, k-1]\n",
    "        \n",
    "        # Ensure non-negative (numerical stability)\n",
    "        class_probs = np.maximum(class_probs, 0)\n",
    "        row_sums = class_probs.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = class_probs / row_sums\n",
    "        \n",
    "        return class_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probs, axis=1)]\n",
    "\n",
    "print(\"âœ… Ordinal Logistic Regression class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Ordinal Logistic Regression...\")\n",
    "\n",
    "olr_model = OrdinalLogisticRegression(max_iter=1000)\n",
    "olr_model.fit(X_train, y_train)\n",
    "olr_pred = olr_model.predict(X_test)\n",
    "\n",
    "olr_results = evaluate_model(y_test, olr_pred, \"Ordinal Logistic Regression\")\n",
    "olr_adjacent = calculate_adjacent_error_rate(y_test, olr_pred)\n",
    "olr_severe = calculate_severe_error_rate(y_test, olr_pred)\n",
    "print(f\"Adjacent Error Rate: {olr_adjacent:.2%}\")\n",
    "print(f\"Severe Error Rate:   {olr_severe:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, olr_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = pd.DataFrame([\n",
    "    {**nb_results, 'encoding': 'Nominal', 'adjacent_error': nb_adjacent, 'severe_error': nb_severe},\n",
    "    {**lr_nom_results, 'encoding': 'Nominal', 'adjacent_error': lr_nom_adjacent, 'severe_error': lr_nom_severe},\n",
    "    {**ridge_results, 'encoding': 'Ordinal', 'adjacent_error': ridge_adjacent, 'severe_error': ridge_severe},\n",
    "    {**olr_results, 'encoding': 'Ordinal', 'adjacent_error': olr_adjacent, 'severe_error': olr_severe},\n",
    "])\n",
    "\n",
    "all_results = all_results[['model', 'encoding', 'accuracy', 'mae', 'mse', 'f1_macro', 'f1_weighted', 'adjacent_error', 'severe_error']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "display(all_results)\n",
    "\n",
    "# Save results\n",
    "all_results.to_csv('model_results.csv', index=False)\n",
    "print(\"\\nâœ… Saved: model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models = ['NB\\n(Nominal)', 'LR\\n(Nominal)', 'Ridge\\n(Ordinal)', 'OLR\\n(Ordinal)']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(range(len(models)), all_results['accuracy'], color=colors, edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(models)))\n",
    "axes[0, 0].set_xticklabels(models, fontsize=10)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Accuracy (Higher = Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(all_results['accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].bar(range(len(models)), all_results['mae'], color=colors, edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(models)))\n",
    "axes[0, 1].set_xticklabels(models, fontsize=10)\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error', fontsize=11)\n",
    "axes[0, 1].set_title('MAE (Lower = Better)', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(all_results['mae']):\n",
    "    axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# F1 Macro\n",
    "axes[1, 0].bar(range(len(models)), all_results['f1_macro'], color=colors, edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(models)))\n",
    "axes[1, 0].set_xticklabels(models, fontsize=10)\n",
    "axes[1, 0].set_ylabel('F1 Score (Macro)', fontsize=11)\n",
    "axes[1, 0].set_title('F1 Macro (Higher = Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(all_results['f1_macro']):\n",
    "    axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Error Types\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, all_results['adjacent_error'], width, label='Adjacent (Â±1)', color='#f39c12', edgecolor='black')\n",
    "axes[1, 1].bar(x + width/2, all_results['severe_error'], width, label='Severe (Â±2+)', color='#c0392b', edgecolor='black')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(models, fontsize=10)\n",
    "axes[1, 1].set_ylabel('Error Rate', fontsize=11)\n",
    "axes[1, 1].set_title('Error Type Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "predictions = [\n",
    "    (nb_pred, \"Naive Bayes (Nominal)\"),\n",
    "    (lr_nom_pred, \"Logistic Regression (Nominal)\"),\n",
    "    (ridge_pred, \"Ridge Regression (Ordinal)\"),\n",
    "    (olr_pred, \"Ordinal Logistic Regression\")\n",
    "]\n",
    "\n",
    "for ax, (pred, name) in zip(axes.flat, predictions):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=[1, 2, 3, 4, 5],\n",
    "                yticklabels=[1, 2, 3, 4, 5])\n",
    "    ax.set_xlabel('Predicted Rating', fontsize=11)\n",
    "    ax.set_ylabel('Actual Rating', fontsize=11)\n",
    "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_accuracy = all_results.loc[all_results['accuracy'].idxmax()]\n",
    "best_mae = all_results.loc[all_results['mae'].idxmin()]\n",
    "lowest_severe = all_results.loc[all_results['severe_error'].idxmin()]\n",
    "\n",
    "print(f\"\\n1. BEST ACCURACY: {best_accuracy['model']}\")\n",
    "print(f\"   Accuracy: {best_accuracy['accuracy']:.4f} ({best_accuracy['accuracy']*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n2. LOWEST MAE: {best_mae['model']}\")\n",
    "print(f\"   MAE: {best_mae['mae']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. LOWEST SEVERE ERROR RATE: {lowest_severe['model']}\")\n",
    "print(f\"   Severe Error Rate: {lowest_severe['severe_error']:.2%}\")\n",
    "\n",
    "# Ordinal vs Nominal comparison\n",
    "nominal_avg_mae = all_results[all_results['encoding'] == 'Nominal']['mae'].mean()\n",
    "ordinal_avg_mae = all_results[all_results['encoding'] == 'Ordinal']['mae'].mean()\n",
    "improvement = (nominal_avg_mae - ordinal_avg_mae) / nominal_avg_mae * 100\n",
    "\n",
    "print(f\"\\n4. ORDINAL VS NOMINAL:\")\n",
    "print(f\"   Nominal Avg MAE: {nominal_avg_mae:.4f}\")\n",
    "print(f\"   Ordinal Avg MAE: {ordinal_avg_mae:.4f}\")\n",
    "print(f\"   MAE Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - download files\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    files.download('class_distribution.png')\n",
    "    files.download('model_comparison.png')\n",
    "    files.download('confusion_matrices.png')\n",
    "    files.download('model_results.csv')\n",
    "    print(\"âœ… Files downloaded!\")\n",
    "except:\n",
    "    print(\"Not in Colab - files saved locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
