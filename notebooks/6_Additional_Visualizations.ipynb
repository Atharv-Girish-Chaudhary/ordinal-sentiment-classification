{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 6: Additional Visualizations\n",
    "## Final Project - Ordinal vs Nominal Sentiment Analysis\n",
    "\n",
    "**Purpose:** Generate publication-quality visualizations for the IEEE report\n",
    "\n",
    "**Input:** Results from Notebooks 3, 4, and 5\n",
    "\n",
    "**Output:** 8 high-quality figures for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/Fall 2025/Foundations of Artificial Intelligence/Final Project/figures/'\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "print(f\"‚úÖ Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS DATA FROM YOUR EXPERIMENTS\n",
    "# =============================================================================\n",
    "\n",
    "# Model results\n",
    "models_data = {\n",
    "    'Model': ['Naive Bayes', 'Logistic Regression', 'Ridge Regression', 'Ordinal Logistic Regression'],\n",
    "    'Model_Short': ['NB', 'LR', 'Ridge', 'OLR'],\n",
    "    'Encoding': ['Nominal', 'Nominal', 'Ordinal', 'Ordinal'],\n",
    "    'Accuracy': [0.6312, 0.6595, 0.5029, 0.6586],\n",
    "    'MAE': [0.6651, 0.5337, 0.6055, 0.5360],\n",
    "    'F1_Macro': [0.2321, 0.3793, 0.3063, 0.3702],\n",
    "    'F1_Weighted': [0.5129, 0.6064, 0.5244, 0.6049],\n",
    "    'Adjacent_Error': [0.5563, 0.6517, 0.8192, 0.6526],\n",
    "    'Severe_Error': [0.4437, 0.3483, 0.1808, 0.3474]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(models_data)\n",
    "\n",
    "print(\"üìä Model Results:\")\n",
    "print(\"=\"*80)\n",
    "display(df_results[['Model', 'Encoding', 'Accuracy', 'MAE', 'F1_Macro', 'Severe_Error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores from classification reports\n",
    "per_class_f1 = {\n",
    "    'Rating': [1, 2, 3, 4, 5],\n",
    "    'Naive Bayes': [0.2980, 0.0000, 0.0075, 0.0797, 0.7751],\n",
    "    'Logistic Regression': [0.5098, 0.1006, 0.1800, 0.3005, 0.8054],\n",
    "    'Ridge Regression': [0.1256, 0.1326, 0.2408, 0.3507, 0.6817],\n",
    "    'Ordinal LR': [0.4332, 0.1141, 0.1993, 0.2964, 0.8080]\n",
    "}\n",
    "\n",
    "df_f1 = pd.DataFrame(per_class_f1)\n",
    "\n",
    "print(\"\\nüìä Per-Class F1 Scores:\")\n",
    "print(\"=\"*80)\n",
    "display(df_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_dist = {\n",
    "    'Rating': [1, 2, 3, 4, 5],\n",
    "    'Count': [2835, 2161, 3964, 10103, 30897],\n",
    "    'Percentage': [5.7, 4.3, 7.9, 20.2, 61.8]\n",
    "}\n",
    "\n",
    "df_class = pd.DataFrame(class_dist)\n",
    "\n",
    "print(\"\\nüìä Class Distribution:\")\n",
    "print(\"=\"*80)\n",
    "display(df_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices from your results\n",
    "# Logistic Regression (Nominal) - Best accuracy\n",
    "cm_lr = np.array([\n",
    "    [248, 28, 46, 40, 205],\n",
    "    [63, 26, 57, 79, 207],\n",
    "    [45, 14, 97, 176, 461],\n",
    "    [22, 6, 50, 473, 1470],\n",
    "    [28, 11, 35, 359, 5746]\n",
    "])\n",
    "\n",
    "# Ridge Regression (Ordinal) - Best severe error\n",
    "cm_ridge = np.array([\n",
    "    [39, 140, 241, 135, 12],\n",
    "    [10, 46, 187, 177, 12],\n",
    "    [5, 48, 219, 430, 91],\n",
    "    [0, 14, 182, 1083, 742],\n",
    "    [0, 14, 197, 2330, 3638]\n",
    "])\n",
    "\n",
    "# Naive Bayes (Nominal)\n",
    "cm_nb = np.array([\n",
    "    [111, 0, 1, 14, 441],\n",
    "    [22, 0, 0, 22, 388],\n",
    "    [17, 0, 3, 51, 722],\n",
    "    [14, 0, 2, 90, 1915],\n",
    "    [14, 0, 2, 60, 6103]\n",
    "])\n",
    "\n",
    "# Ordinal Logistic Regression\n",
    "cm_olr = np.array([\n",
    "    [188, 50, 61, 36, 232],\n",
    "    [51, 32, 71, 78, 200],\n",
    "    [30, 27, 114, 167, 455],\n",
    "    [14, 10, 64, 456, 1477],\n",
    "    [18, 10, 41, 319, 5791]\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Confusion matrices loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 1: Per-Class F1 Score Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = df_f1.set_index('Rating').T\n",
    "heatmap_data.index = ['Naive Bayes\\n(Nominal)', 'Logistic Reg\\n(Nominal)', \n",
    "                       'Ridge Reg\\n(Ordinal)', 'Ordinal LR\\n(Ordinal)']\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            center=0.4, vmin=0, vmax=0.85,\n",
    "            cbar_kws={'label': 'F1 Score'},\n",
    "            linewidths=0.5, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Star Rating', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class F1 Scores Across Models', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(['1-Star', '2-Star', '3-Star', '4-Star', '5-Star'], fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_1_f1_heatmap.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_1_f1_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 2: Error Type Distribution (Stacked Bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Naive Bayes\\n(Nominal)', 'Logistic Reg\\n(Nominal)', \n",
    "          'Ridge Reg\\n(Ordinal)', 'Ordinal LR\\n(Ordinal)']\n",
    "\n",
    "# Calculate correct, adjacent error, and severe error percentages\n",
    "correct = [63.12, 65.95, 50.29, 65.86]\n",
    "# Adjacent errors (of the incorrect predictions)\n",
    "adjacent_pct = [55.63, 65.17, 81.92, 65.26]\n",
    "severe_pct = [44.37, 34.83, 18.08, 34.74]\n",
    "\n",
    "# Convert to absolute percentages\n",
    "incorrect = [100 - c for c in correct]\n",
    "adjacent = [inc * adj / 100 for inc, adj in zip(incorrect, adjacent_pct)]\n",
    "severe = [inc * sev / 100 for inc, sev in zip(incorrect, severe_pct)]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.6\n",
    "\n",
    "# Stacked bar\n",
    "bars1 = ax.bar(x, correct, width, label='Correct', color='#27ae60', edgecolor='black')\n",
    "bars2 = ax.bar(x, adjacent, width, bottom=correct, label='Adjacent Error (¬±1)', \n",
    "               color='#f39c12', edgecolor='black')\n",
    "bars3 = ax.bar(x, severe, width, bottom=[c+a for c,a in zip(correct, adjacent)], \n",
    "               label='Severe Error (¬±2+)', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Percentage of Predictions', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Prediction Outcome Distribution by Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=10)\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (c, a, s) in enumerate(zip(correct, adjacent, severe)):\n",
    "    ax.text(i, c/2, f'{c:.1f}%', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(i, c + a/2, f'{a:.1f}%', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    ax.text(i, c + a + s/2, f'{s:.1f}%', ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_2_error_distribution.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_2_error_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 3: Radar Chart - Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Metrics (normalized to 0-1 scale where higher is better)\n",
    "categories = ['Accuracy', 'F1 Macro', '1 - MAE', '1 - Severe\\nError', 'Minority Class\\nRecall']\n",
    "\n",
    "# Data for each model (normalized so higher = better)\n",
    "nb_vals = [0.6312, 0.2321, 1-0.6651, 1-0.4437, 0.2980]\n",
    "lr_vals = [0.6595, 0.3793, 1-0.5337, 1-0.3483, 0.5098]\n",
    "ridge_vals = [0.5029, 0.3063, 1-0.6055, 1-0.1808, 0.1256]\n",
    "olr_vals = [0.6586, 0.3702, 1-0.5360, 1-0.3474, 0.4332]\n",
    "\n",
    "# Number of variables\n",
    "N = len(categories)\n",
    "\n",
    "# Compute angle for each category\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "# Add values (close the loop)\n",
    "nb_vals += nb_vals[:1]\n",
    "lr_vals += lr_vals[:1]\n",
    "ridge_vals += ridge_vals[:1]\n",
    "olr_vals += olr_vals[:1]\n",
    "\n",
    "# Plot\n",
    "ax.plot(angles, nb_vals, 'o-', linewidth=2, label='Naive Bayes (Nominal)', color='#3498db')\n",
    "ax.fill(angles, nb_vals, alpha=0.1, color='#3498db')\n",
    "\n",
    "ax.plot(angles, lr_vals, 's-', linewidth=2, label='Logistic Reg (Nominal)', color='#2ecc71')\n",
    "ax.fill(angles, lr_vals, alpha=0.1, color='#2ecc71')\n",
    "\n",
    "ax.plot(angles, ridge_vals, '^-', linewidth=2, label='Ridge Reg (Ordinal)', color='#e74c3c')\n",
    "ax.fill(angles, ridge_vals, alpha=0.1, color='#e74c3c')\n",
    "\n",
    "ax.plot(angles, olr_vals, 'd-', linewidth=2, label='Ordinal LR (Ordinal)', color='#9b59b6')\n",
    "ax.fill(angles, olr_vals, alpha=0.1, color='#9b59b6')\n",
    "\n",
    "# Set category labels\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Multi-Metric Model Comparison\\n(Higher = Better)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.35, 1.0), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_3_radar_comparison.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_3_radar_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 4: Severe Error Rate Comparison (KEY FINDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Naive Bayes\\n(Nominal)', 'Logistic Reg\\n(Nominal)', \n",
    "          'Ridge Reg\\n(Ordinal)', 'Ordinal LR\\n(Ordinal)']\n",
    "severe_rates = [44.37, 34.83, 18.08, 34.74]\n",
    "colors = ['#3498db', '#3498db', '#27ae60', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(models, severe_rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Highlight the best (lowest) severe error\n",
    "bars[2].set_edgecolor('#1e8449')\n",
    "bars[2].set_linewidth(3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, severe_rates):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{rate:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add annotation for best model\n",
    "ax.annotate('BEST: 48% reduction\\nvs Nominal average', \n",
    "            xy=(2, 18.08), xytext=(2.5, 35),\n",
    "            fontsize=11, ha='center',\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2),\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#d5f5e3', edgecolor='#27ae60'))\n",
    "\n",
    "ax.set_ylabel('Severe Error Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Severe Misclassification Rate (¬±2 or more stars off)\\nLower is Better', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 55)\n",
    "\n",
    "# Add nominal average line\n",
    "nominal_avg = (44.37 + 34.83) / 2\n",
    "ax.axhline(y=nominal_avg, color='gray', linestyle='--', alpha=0.7, linewidth=2)\n",
    "ax.text(3.5, nominal_avg + 1.5, f'Nominal Avg: {nominal_avg:.1f}%', fontsize=10, color='gray')\n",
    "\n",
    "# Add color legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#3498db', edgecolor='black', label='Nominal Encoding'),\n",
    "                   Patch(facecolor='#e74c3c', edgecolor='black', label='Ordinal Encoding'),\n",
    "                   Patch(facecolor='#27ae60', edgecolor='black', label='Best Model')]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_4_severe_error.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_4_severe_error.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 5: Accuracy vs MAE Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Data\n",
    "models_short = ['NB', 'LR', 'Ridge', 'OLR']\n",
    "accuracy = [63.12, 65.95, 50.29, 65.86]\n",
    "mae = [0.6651, 0.5337, 0.6055, 0.5360]\n",
    "severe = [44.37, 34.83, 18.08, 34.74]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "markers = ['o', 's', '^', 'd']\n",
    "\n",
    "# Scatter plot with size proportional to (100 - severe error)\n",
    "for i, (acc, m, sev, c, mark, name) in enumerate(zip(accuracy, mae, severe, colors, markers, models_short)):\n",
    "    size = (100 - sev) * 10  # Size proportional to low severe error\n",
    "    ax.scatter(acc, m, s=size, c=c, marker=mark, edgecolor='black', linewidth=1.5,\n",
    "               label=f'{name}', alpha=0.8)\n",
    "\n",
    "# Add annotations\n",
    "annotations = [\n",
    "    ('Naive Bayes\\n(Nominal)', (63.12, 0.6651), (-60, 20)),\n",
    "    ('Logistic Reg\\n(Nominal)', (65.95, 0.5337), (40, 30)),\n",
    "    ('Ridge Reg\\n(Ordinal)', (50.29, 0.6055), (-70, -30)),\n",
    "    ('Ordinal LR\\n(Ordinal)', (65.86, 0.5360), (40, -40))\n",
    "]\n",
    "\n",
    "for name, (x, y), offset in annotations:\n",
    "    ax.annotate(name, (x, y), xytext=offset, textcoords='offset points',\n",
    "                fontsize=10, ha='center',\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7),\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.9))\n",
    "\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Accuracy vs MAE Trade-off\\n(Bubble size proportional to Low Severe Error Rate)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Ideal region annotation\n",
    "ax.annotate('Ideal Region\\n(High Acc, Low MAE)', xy=(68, 0.48), fontsize=10,\n",
    "            style='italic', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xlim(45, 72)\n",
    "ax.set_ylim(0.45, 0.75)\n",
    "ax.invert_yaxis()  # Lower MAE is better, so invert\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_5_accuracy_vs_mae.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_5_accuracy_vs_mae.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 6: Class Imbalance Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Class distribution\n",
    "ratings = ['1-Star', '2-Star', '3-Star', '4-Star', '5-Star']\n",
    "counts = [2835, 2161, 3964, 10103, 30897]\n",
    "colors_dist = ['#e74c3c', '#e74c3c', '#f39c12', '#3498db', '#27ae60']\n",
    "\n",
    "bars1 = ax1.bar(ratings, counts, color=colors_dist, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Reviews', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Star Rating', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Class Distribution\\n(Highly Imbalanced)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, count in zip(bars1, counts):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 500,\n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: F1 scores by class for best models\n",
    "x = np.arange(5)\n",
    "width = 0.35\n",
    "\n",
    "f1_lr = [0.5098, 0.1006, 0.1800, 0.3005, 0.8054]\n",
    "f1_olr = [0.4332, 0.1141, 0.1993, 0.2964, 0.8080]\n",
    "\n",
    "bars2 = ax2.bar(x - width/2, f1_lr, width, label='Logistic Reg (Nominal)', \n",
    "                color='#2ecc71', edgecolor='black')\n",
    "bars3 = ax2.bar(x + width/2, f1_olr, width, label='Ordinal LR (Ordinal)', \n",
    "                color='#9b59b6', edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Star Rating', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Per-Class F1 Scores\\n(Best Nominal vs Best Ordinal)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ratings)\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add annotation\n",
    "ax2.annotate('Minority classes (1-3 stars)\\nhave lowest F1 scores', \n",
    "             xy=(1, 0.15), xytext=(2.8, 0.5),\n",
    "             fontsize=10, ha='center',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='#fadbd8', edgecolor='#e74c3c'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_6_imbalance_impact.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_6_imbalance_impact.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 7: Summary Figure (MAIN FIGURE FOR REPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Create grid\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models_short = ['NB\\n(Nom)', 'LR\\n(Nom)', 'Ridge\\n(Ord)', 'OLR\\n(Ord)']\n",
    "accuracy = [63.12, 65.95, 50.29, 65.86]\n",
    "colors = ['#3498db', '#27ae60', '#e74c3c', '#9b59b6']\n",
    "bars = ax1.bar(models_short, accuracy, color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('A) Classification Accuracy', fontweight='bold')\n",
    "ax1.set_ylim(0, 80)\n",
    "for bar, acc in zip(bars, accuracy):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "             f'{acc:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: MAE comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "mae = [0.6651, 0.5337, 0.6055, 0.5360]\n",
    "bars = ax2.bar(models_short, mae, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('MAE (Lower = Better)')\n",
    "ax2.set_title('B) Mean Absolute Error', fontweight='bold')\n",
    "ax2.set_ylim(0, 0.8)\n",
    "for bar, m in zip(bars, mae):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "             f'{m:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 3: Severe Error comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "severe = [44.37, 34.83, 18.08, 34.74]\n",
    "bar_colors = ['#3498db', '#3498db', '#27ae60', '#e74c3c']\n",
    "bars = ax3.bar(models_short, severe, color=bar_colors, edgecolor='black')\n",
    "ax3.set_ylabel('Severe Error Rate (%)')\n",
    "ax3.set_title('C) Severe Misclassification (¬±2+)', fontweight='bold')\n",
    "ax3.set_ylim(0, 55)\n",
    "for bar, s in zip(bars, severe):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "             f'{s:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 4: Ordinal vs Nominal summary\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "metrics = ['Accuracy', 'MAE', 'Severe\\nError']\n",
    "# Normalize for comparison (scale 0-100)\n",
    "nominal_avg = [64.54, 59.94, 39.60]  # MAE scaled, Severe as-is\n",
    "ordinal_avg = [58.08, 57.08, 26.41]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, nominal_avg, width, label='Nominal (avg)', color='#3498db', edgecolor='black')\n",
    "ax4.bar(x + width/2, ordinal_avg, width, label='Ordinal (avg)', color='#e74c3c', edgecolor='black')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.set_title('D) Encoding Comparison', fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.set_ylabel('Score')\n",
    "\n",
    "# Plot 5: Key findings text box\n",
    "ax5 = fig.add_subplot(gs[1, 1:])\n",
    "ax5.axis('off')\n",
    "\n",
    "findings_text = \"\"\"KEY FINDINGS\n",
    "\n",
    "Research Question: Do performance gains from ordinal treatment \n",
    "of 5-star ratings justify the increased model complexity?\n",
    "\n",
    "1. ACCURACY: Nominal encoding achieves highest accuracy\n",
    "   ‚Üí Logistic Regression (Nominal): 65.95%\n",
    "\n",
    "2. SEVERE ERROR REDUCTION: Ordinal encoding dramatically \n",
    "   reduces severe misclassifications\n",
    "   ‚Üí Ridge Regression (Ordinal): Only 18.08% severe error\n",
    "   ‚Üí 48% reduction compared to nominal average (39.60%)\n",
    "\n",
    "3. TRADE-OFF: Ordinal models sacrifice some accuracy for \n",
    "   better error distribution. When misclassifying, ordinal \n",
    "   models stay closer to the true rating.\n",
    "\n",
    "4. PRACTICAL IMPLICATION: Model choice depends on application\n",
    "   ‚Üí Exact accuracy matters ‚Üí Use Nominal (Logistic Regression)\n",
    "   ‚Üí Avoiding large errors matters ‚Üí Use Ordinal (Ridge)\n",
    "\n",
    "5. CLASS IMBALANCE: Minority classes (1-3 stars) have poor \n",
    "   F1 scores across all models due to 14:1 imbalance ratio.\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.02, 0.98, findings_text, transform=ax5.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='#f8f9fa', edgecolor='#dee2e6', pad=0.5))\n",
    "\n",
    "plt.suptitle('Ordinal vs Nominal Sentiment Classification: Summary of Results', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_7_summary.png', facecolor='white', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_7_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 8: Confusion Matrix Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Confusion matrices\n",
    "cms = [cm_nb, cm_lr, cm_ridge, cm_olr]\n",
    "titles = [\n",
    "    'Naive Bayes (Nominal)\\nAccuracy: 63.12% | Severe Error: 44.37%',\n",
    "    'Logistic Regression (Nominal)\\nAccuracy: 65.95% | Severe Error: 34.83%',\n",
    "    'Ridge Regression (Ordinal)\\nAccuracy: 50.29% | Severe Error: 18.08%',\n",
    "    'Ordinal Logistic Regression\\nAccuracy: 65.86% | Severe Error: 34.74%'\n",
    "]\n",
    "cmaps = ['Blues', 'Greens', 'Oranges', 'Purples']\n",
    "\n",
    "for ax, cm, title, cmap in zip(axes.flat, cms, titles, cmaps):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
    "                xticklabels=['1', '2', '3', '4', '5'],\n",
    "                yticklabels=['1', '2', '3', '4', '5'])\n",
    "    ax.set_xlabel('Predicted Rating', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Actual Rating', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices: All Models Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_8_confusion_matrices.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_8_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Visualization 9: Best vs Best Comparison (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Logistic Regression (Best Accuracy)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=['1', '2', '3', '4', '5'],\n",
    "            yticklabels=['1', '2', '3', '4', '5'])\n",
    "axes[0].set_xlabel('Predicted Rating', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Rating', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Logistic Regression (Nominal)\\nBest Accuracy: 65.95%', \n",
    "                  fontsize=12, fontweight='bold', color='#27ae60')\n",
    "\n",
    "# Plot Ridge Regression (Best Severe Error)\n",
    "sns.heatmap(cm_ridge, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['1', '2', '3', '4', '5'],\n",
    "            yticklabels=['1', '2', '3', '4', '5'])\n",
    "axes[1].set_xlabel('Predicted Rating', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Rating', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Ridge Regression (Ordinal)\\nLowest Severe Error: 18.08%', \n",
    "                  fontsize=12, fontweight='bold', color='#e74c3c')\n",
    "\n",
    "plt.suptitle('Best Accuracy Model vs Best Severe Error Model', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_PATH}fig_9_best_vs_best.png', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: fig_9_best_vs_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä ALL VISUALIZATIONS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput folder: {OUTPUT_PATH}\")\n",
    "print(\"\\nFigures generated:\")\n",
    "print(\"-\"*70)\n",
    "print(\"1. fig_1_f1_heatmap.png        - Per-class F1 scores heatmap\")\n",
    "print(\"2. fig_2_error_distribution.png - Error type breakdown (stacked)\")\n",
    "print(\"3. fig_3_radar_comparison.png   - Multi-metric radar chart\")\n",
    "print(\"4. fig_4_severe_error.png       - Severe error comparison ‚≠ê KEY\")\n",
    "print(\"5. fig_5_accuracy_vs_mae.png    - Trade-off scatter plot\")\n",
    "print(\"6. fig_6_imbalance_impact.png   - Class imbalance analysis\")\n",
    "print(\"7. fig_7_summary.png            - Complete summary ‚≠ê MAIN\")\n",
    "print(\"8. fig_8_confusion_matrices.png - All 4 confusion matrices\")\n",
    "print(\"9. fig_9_best_vs_best.png       - Best accuracy vs best error\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nüìù Recommended for IEEE Report:\")\n",
    "print(\"   ‚Ä¢ Figure 1: Class distribution (from Notebook 2)\")\n",
    "print(\"   ‚Ä¢ Figure 2: fig_7_summary.png (main results)\")\n",
    "print(\"   ‚Ä¢ Figure 3: fig_9_best_vs_best.png (confusion matrices)\")\n",
    "print(\"   ‚Ä¢ Figure 4: fig_4_severe_error.png (key finding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all figures\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# List all generated files\n",
    "files = [f for f in os.listdir(OUTPUT_PATH) if f.startswith('fig_')]\n",
    "print(f\"\\nüìÅ Files in output folder ({len(files)} figures):\")\n",
    "for f in sorted(files):\n",
    "    print(f\"   ‚Ä¢ {f}\")\n",
    "\n",
    "# Optional: Download to local\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    print(\"\\nüì• Downloading figures...\")\n",
    "    for f in sorted(files):\n",
    "        colab_files.download(os.path.join(OUTPUT_PATH, f))\n",
    "    print(\"‚úÖ Downloads complete!\")\n",
    "except:\n",
    "    print(\"\\nüìÇ Files saved to Google Drive. Access them at:\")\n",
    "    print(f\"   {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
